{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced10ec9",
   "metadata": {},
   "source": [
    "### Markov Chain to predict the stock market, based on the post from Pranab Gosh \"Customer Conersion Prediction with Markov Chain Classifier\" (https://pkghosh.wordpress.com/2015/07/06/customer-conversion-prediction-with-markov-chain-classifier/) \n",
    "### -->  binary classification with two transition matrices (first ordered matrix), positive and negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753e278",
   "metadata": {},
   "source": [
    "### 1) First-Order Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert image using python package\n",
    "from IPython.display import Image\n",
    "Image(filename=\"./first-order-matrix.png\", width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ffa353",
   "metadata": {},
   "source": [
    "### 2) Gataloging Patterns Using Market Data\n",
    "#### &#x23f5; 10 years of S&P 500 index data represents only one sequence of many events leading to the last quoted price. Breaking data into may samples of sequences leading to different price patterns for model to learn richer and diverse patterns, I use the moving average to understand this\n",
    "\n",
    "### 3) example\n",
    "#### &#x23f5; 2012-10-18 to 2012-11-21 \n",
    "1417.26 -> 1428.39 -> 1394.53 -> 1377.51 -> Next Day Volume Up\n",
    "#### &#x23f5; 2016-08-12 to 2016-08-22\n",
    "2184.05 -> 2190.15 -> 2178.15 -> 2182.22 -> 2187.02 -> Next Day Volume Up \n",
    "#### &#x23f5; 2014-04-04 to 2014-04-10\n",
    "1865.09 -> 1854.04 -> Next Day Volume Down\n",
    "\n",
    "### 4) if any similar price up and down has found compared to historical data in the current dataset, I consider that's a pattern\n",
    "### 5) Binning Values into n(3) Buckets\n",
    "In Pranab Ghosh's approach is to simplify each even within a sequence into a single feature. He split the value into 3 groups - Low, Medium, High. The precent difference between one day's price and the previous day's. Once we have collected all of them, binning them into three groups of equal frequency(number of appearance?) using InfoTheo package.\n",
    "### 6) example\n",
    "#### &#x23f5; closes, opens, highs, lows, we'll end up with a feature containing four letters: \"MLHL\" for example\n",
    "#### &#x23f5; String all the feature events for the sequence and end up with something like this along with the observed outcome: \"HMLL\" -> \"MHHL\" -> \"LLLH\" -> \"HMMM\" -> Volume Up\n",
    "### 7) Creating Two Markov Chains, One for Days with Volume Jumps, and another for Volume Drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a48571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import io, base64, os, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader.data import DataReader\n",
    "import datetime\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a679c",
   "metadata": {},
   "source": [
    "### &#x23fa; Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data extraction\n",
    "start_date = \"2012-12-01\"\n",
    "end_date = \"2022-12-01\"\n",
    "symbol = \"SPY\"\n",
    "data = DataReader(name=symbol, data_source=\"yahoo\", start=start_date, end=end_date)\n",
    "data = data[[\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c19be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add return and range \n",
    "df = data.copy()\n",
    "df[\"Returns\"] = (df[\"Adj Close\"] / df[\"Adj Close\"].shift(1)) - 1 # because latter - previous will cause the last row has no one to divide with, so exclude that row\n",
    "df[\"Range\"] = (df[\"High\"] / df[\"Low\"]) - 1\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all the columns, since Date is not listed as a column, so it should have been recongized as index, we need make it one of the attributes\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index \n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3688e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Date columns into datetime type \n",
    "df[\"Date\"]  = pd.to_datetime(df[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464992d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1510e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take random sets of sequential rows from the stock price with certain pattern\n",
    "new_set = []\n",
    "for row_set in range(0, 100000):\n",
    "    if row_set%2000==0: print(row_set)\n",
    "    row_quant = randint(10, 30)\n",
    "    row_start = randint(0, len(df)-row_quant)\n",
    "    market_subset = df.iloc[row_start:row_start+row_quant]\n",
    "\n",
    "    Close_Date = max(market_subset['Date'])\n",
    "    if row_set%2000==0: print(Close_Date)\n",
    "    \n",
    "    # Close_Gap = (market_subset['Close'] - market_subset['Close'].shift(1)) / market_subset['Close'].shift(1)\n",
    "    Close_Gap = market_subset['Adj Close'].pct_change()\n",
    "    High_Gap = market_subset['High'].pct_change()\n",
    "    Low_Gap = market_subset['Low'].pct_change() \n",
    "    Volume_Gap = market_subset['Volume'].pct_change() \n",
    "    Daily_Change = (market_subset['Adj Close'] - market_subset['Open']) / market_subset['Open']\n",
    "    Outcome_Next_Day_Direction = (market_subset['Volume'].shift(-1) - market_subset['Volume'])\n",
    "    \n",
    "    new_set.append(pd.DataFrame({'Sequence_ID':[row_set]*len(market_subset),\n",
    "                            'Close_Date':[Close_Date]*len(market_subset),\n",
    "                           'Close_Gap':Close_Gap,\n",
    "                           'High_Gap':High_Gap,\n",
    "                           'Low_Gap':Low_Gap,\n",
    "                           'Volume_Gap':Volume_Gap,\n",
    "                           'Daily_Change':Daily_Change,\n",
    "                           'Outcome_Next_Day_Direction':Outcome_Next_Day_Direction}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(market_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa49c77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_set_df = pd.concat(new_set)\n",
    "print(new_set_df.shape)\n",
    "new_set_df = new_set_df.dropna(how='any') \n",
    "print(new_set_df.shape)\n",
    "new_set_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ef900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset\n",
    "new_set_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm sequence\n",
    "# new_set_df[new_set_df['Close_Date'] == '1973-06-27'] {HLH, HLH, HHH, HHH, LLL, LML, LML, LLL, LHL, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d4645",
   "metadata": {},
   "source": [
    "### creating new sequence of dataset for transfer sequential data into categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d10607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences\n",
    "# simplify the data by binning values into three groups\n",
    " \n",
    "# Close_Gap\n",
    "new_set_df['Close_Gap_LMH'] = pd.qcut(new_set_df['Close_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "\n",
    "# High_Gap - not used in this example\n",
    "new_set_df['High_Gap_LMH'] = pd.qcut(new_set_df['High_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "\n",
    "# Low_Gap - not used in this example\n",
    "new_set_df['Low_Gap_LMH'] = pd.qcut(new_set_df['Low_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "\n",
    "# Volume_Gap\n",
    "new_set_df['Volume_Gap_LMH'] = pd.qcut(new_set_df['Volume_Gap'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    " \n",
    "# Daily_Change\n",
    "new_set_df['Daily_Change_LMH'] = pd.qcut(new_set_df['Daily_Change'], 3, labels=[\"L\", \"M\", \"H\"])\n",
    "\n",
    "# new set\n",
    "new_set_df = new_set_df[[\"Sequence_ID\", \n",
    "                         \"Close_Date\", \n",
    "                         \"Close_Gap_LMH\", \n",
    "                         \"Volume_Gap_LMH\", \n",
    "                         \"Daily_Change_LMH\", \n",
    "                         \"Outcome_Next_Day_Direction\"]]\n",
    "\n",
    "new_set_df['Event_Pattern'] = new_set_df['Close_Gap_LMH'].astype(str) + new_set_df['Volume_Gap_LMH'].astype(str) + new_set_df['Daily_Change_LMH'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a81fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "new_set_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6002b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_set_df[\"Outcome_Next_Day_Direction\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51626539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the set\n",
    "compressed_set = new_set_df.groupby(['Sequence_ID', \n",
    "                                     'Close_Date'])['Event_Pattern'].apply(lambda x: \"{%s}\" % ', '.join(x)).reset_index()\n",
    "\n",
    "print(compressed_set.shape)\n",
    "compressed_set.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compressed_outcomes = new_set_df[['Sequence_ID', 'Close_Date', 'Outcome_Next_Day_Direction']].groupby(['Sequence_ID', 'Close_Date']).agg()\n",
    "\n",
    "compressed_outcomes = new_set_df.groupby(['Sequence_ID', 'Close_Date'])['Outcome_Next_Day_Direction'].mean()\n",
    "compressed_outcomes = compressed_outcomes.to_frame().reset_index()\n",
    "print(compressed_outcomes.shape)\n",
    "compressed_outcomes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5202ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    " compressed_set = pd.merge(compressed_set, compressed_outcomes, on= ['Sequence_ID', 'Close_Date'], how='inner')\n",
    "print(compressed_set.shape)\n",
    "compressed_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c77196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reduce set  again\n",
    "# compressed_set = new_set_df.groupby(['Sequence_ID', 'Close_Date','Outcome_Next_Day_Direction'])['Event_Pattern'].apply(lambda x: \"{%s}\" % ', '.join(x)).reset_index()\n",
    "\n",
    "compressed_set['Event_Pattern'] = [''.join(e.split()).replace('{','')\n",
    "                                   .replace('}','') for e in compressed_set['Event_Pattern'].values]\n",
    "compressed_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use last x days of data for validation, setting it as pattern recognize\n",
    "compressed_set_validation = compressed_set[compressed_set['Close_Date'] >= datetime.datetime.now() \n",
    "                                           - datetime.timedelta(days=90)] # Sys.Date()-90 \n",
    "\n",
    "compressed_set_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b019e59",
   "metadata": {},
   "source": [
    "### check the shape for newly combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_set = compressed_set[compressed_set['Close_Date'] < datetime.datetime.now() \n",
    "                                           - datetime.timedelta(days=90)]  \n",
    "compressed_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a1f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(compressed_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2878b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the field \n",
    "# drop date field\n",
    "compressed_set = compressed_set[['Sequence_ID', 'Event_Pattern','Outcome_Next_Day_Direction']]\n",
    "compressed_set_validation = compressed_set_validation[['Sequence_ID', 'Event_Pattern','Outcome_Next_Day_Direction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5fd305",
   "metadata": {},
   "source": [
    "### Keep build moving(rising or droping) only and build outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_set['Outcome_Next_Day_Direction'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e165b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(compressed_set['Outcome_Next_Day_Direction']))\n",
    "len(compressed_set[abs(compressed_set['Outcome_Next_Day_Direction']) > 10000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89144977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only keep big/interesting moves \n",
    "print('all moves:', len(compressed_set))\n",
    "compressed_set = compressed_set[abs(compressed_set['Outcome_Next_Day_Direction']) > 10000000]\n",
    "compressed_set['Outcome_Next_Day_Direction'] = np.where((compressed_set['Outcome_Next_Day_Direction'] > 0), 1, 0)\n",
    "compressed_set_validation['Outcome_Next_Day_Direction'] = np.where((compressed_set_validation['Outcome_Next_Day_Direction'] > 0), 1, 0)\n",
    "print('big moves only:', len(compressed_set))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa437e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two data sets - won/not won\n",
    "compressed_set_pos = compressed_set[compressed_set['Outcome_Next_Day_Direction']==1][['Sequence_ID', 'Event_Pattern']]\n",
    "print(compressed_set_pos.shape)\n",
    "compressed_set_neg = compressed_set[compressed_set['Outcome_Next_Day_Direction']==0][['Sequence_ID', 'Event_Pattern']]\n",
    "print(compressed_set_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item.split(',') for item in compressed_set['Event_Pattern'].values ]\n",
    "unique_patterns = ','.join(str(r) for v in flat_list for r in v)\n",
    "unique_patterns = list(set(unique_patterns.split(',')))\n",
    "len(unique_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_set['Outcome_Next_Day_Direction'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d998f6",
   "metadata": {},
   "source": [
    "### Build the Markov Chain grid (first ordered matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cc059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the markov transition grid\n",
    "def build_transition_grid(compressed_grid, unique_patterns):\n",
    "    # build the markov transition grid\n",
    "\n",
    "    patterns = []\n",
    "    counts = []\n",
    "    for from_event in unique_patterns:\n",
    "\n",
    "        # how many times \n",
    "        for to_event in unique_patterns:\n",
    "            pattern = from_event + ',' + to_event # MMM,MlM\n",
    "\n",
    "            ids_matches = compressed_grid[compressed_grid['Event_Pattern'].str.contains(pattern)]\n",
    "            found = 0\n",
    "            if len(ids_matches) > 0:\n",
    "                Event_Pattern = '---'.join(ids_matches['Event_Pattern'].values)\n",
    "                found = Event_Pattern.count(pattern)\n",
    "            patterns.append(pattern)\n",
    "            counts.append(found)\n",
    "\n",
    "    # create to/from grid\n",
    "    grid_Df = pd.DataFrame({'pairs':patterns, 'counts': counts})\n",
    "\n",
    "    grid_Df['x'], grid_Df['y'] = grid_Df['pairs'].str.split(',', 1).str\n",
    "    grid_Df.head()\n",
    "\n",
    "    grid_Df = grid_Df.pivot(index='x', columns='y', values='counts')\n",
    "\n",
    "    grid_Df.columns= [col for col in grid_Df.columns]\n",
    "    del grid_Df.index.name\n",
    "\n",
    "    # replace all NaN with zeros\n",
    "    grid_Df.fillna(0, inplace=True)\n",
    "    grid_Df.head()\n",
    "\n",
    "    #grid_Df.rowSums(transition_dataframe) \n",
    "    grid_Df = grid_Df / grid_Df.sum(1)\n",
    "    return (grid_Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pos = build_transition_grid(compressed_set_pos, unique_patterns) \n",
    "grid_neg = build_transition_grid(compressed_set_neg, unique_patterns) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51efcd84",
   "metadata": {},
   "source": [
    "### Separately with positive and negative grid for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc317a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75148a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_set_validation[compressed_set_validation['Sequence_ID' == seq_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x,y):\n",
    "   try:\n",
    "      lg = np.log(x/y)\n",
    "   except:\n",
    "      lg = 0\n",
    "   return lg\n",
    "\n",
    "# predict on out of sample data\n",
    "actual = []\n",
    "predicted = []\n",
    "for seq_id in compressed_set_validation['Sequence_ID'].values:\n",
    "    patterns = compressed_set_validation[compressed_set_validation['Sequence_ID'] == seq_id]['Event_Pattern'].values[0].split(',')\n",
    "    pos = []\n",
    "    neg = []\n",
    "    log_odds = []\n",
    "    \n",
    "    for id in range(0, len(patterns)-1):\n",
    "        # get log odds\n",
    "        # logOdds = log(tp(i,j) / tn(i,j)\n",
    "        if (patterns[id] in list(grid_pos) and patterns[id+1] in list(grid_pos) and patterns[id] in list(grid_neg) and patterns[id+1] in list(grid_neg)):\n",
    "                \n",
    "            numerator = grid_pos[patterns[id]][patterns[id+1]]\n",
    "            denominator = grid_neg[patterns[id]][patterns[id+1]]\n",
    "            if (numerator == 0 and denominator == 0):\n",
    "                log_value =0\n",
    "            elif (denominator == 0):\n",
    "                log_value = np.log(numerator / 0.00001)\n",
    "            elif (numerator == 0):\n",
    "                log_value = np.log(0.00001 / denominator)\n",
    "            else:\n",
    "                log_value = np.log(numerator/denominator)\n",
    "        else:\n",
    "            log_value = 0\n",
    "        \n",
    "        log_odds.append(log_value)\n",
    "        \n",
    "        pos.append(numerator)\n",
    "        neg.append(denominator)\n",
    "      \n",
    "    print('outcome:', compressed_set_validation[compressed_set_validation['Sequence_ID']==seq_id]['Outcome_Next_Day_Direction'].values[0])\n",
    "    print(sum(pos)/sum(neg))\n",
    "    print(sum(log_odds))\n",
    "\n",
    "    actual.append(compressed_set_validation[compressed_set_validation['Sequence_ID']==seq_id]['Outcome_Next_Day_Direction'].values[0])\n",
    "    predicted.append(sum(log_odds))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(actual, [1 if p > 0 else 0 for p in predicted])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc1a67",
   "metadata": {},
   "source": [
    "### Build the confusion matrix for accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(actual, [1 if p > 0 else 0 for p in predicted])\n",
    "print('Accuracy:', round(score * 100,2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cm = confusion_matrix(actual, [1 if p > 0 else 0 for p in predicted])\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt='g')\n",
    "\n",
    "ax.set_title('Confusion Matrix') \n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['up day','down day'])\n",
    "ax.yaxis.set_ticklabels(['up day','down day'])\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation = 0, fontsize = 8)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90, fontsize = 8)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910c95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44474006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from pyhhmm.gaussian import GaussianHMM \n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5eb1d7",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data extraction \n",
    "start_date = '2017-01-1'\n",
    "end_date = '2022-06-01'\n",
    "symbol = \"SPY\"\n",
    "data = DataReader(name=symbol, data_source=\"yahoo\", start=start_date, end=end_date)\n",
    "data = data[[\"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add return and range \n",
    "df = data.copy()\n",
    "df[\"Returns\"] = (df[\"Adj Close\"] / df[\"Adj Close\"].shift(1)) - 1 # because latter - previous value in row level \n",
    "df[\"Range\"] = (df[\"High\"] / df[\"Low\"]) - 1\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure data \n",
    "X_train = df[[\"Returns\", \"Range\"]]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645b9e1",
   "metadata": {},
   "source": [
    "### HMM Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d438441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model \n",
    "model = GaussianHMM(n_states=4, covariance_type='full', n_emissions=2)\n",
    "model.train([np.array(X_train.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results \n",
    "hidden_states = model.predict([X_train.values])[0]\n",
    "hidden_states[:40]\n",
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regime state means for each feature \n",
    "model.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "model.covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e850aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "# dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2a4d6e",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure the prices for plotting \n",
    "i = 0\n",
    "labels_0 = []\n",
    "labels_1 = [] \n",
    "labels_2 = [] \n",
    "labels_3 = []\n",
    "prices = df[\"Adj Close\"].values.astype(float)\n",
    "print(\"Correct number of rows:\", len(prices) == len(hidden_states))\n",
    "for s in hidden_states:\n",
    "    if s == 0:\n",
    "        labels_0.append(prices[i])\n",
    "        labels_1.append(float('nan'))\n",
    "        labels_2.append(float('nan'))\n",
    "        labels_3.append(float('nan'))\n",
    "    if s == 1:\n",
    "        labels_0.append(float('nan'))\n",
    "        labels_1.append(prices[i])\n",
    "        labels_2.append(float('nan'))\n",
    "        labels_3.append(float('nan'))\n",
    "    if s == 2:\n",
    "        labels_0.append(float('nan'))\n",
    "        labels_1.append(float('nan'))\n",
    "        labels_2.append(prices[i])\n",
    "        labels_3.append(float('nan'))\n",
    "    if s == 3:\n",
    "        labels_0.append(float('nan'))\n",
    "        labels_1.append(float('nan'))\n",
    "        labels_2.append(float('nan'))\n",
    "        labels_3.append(prices[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff580a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot chart \n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "plt.plot(labels_0, color=\"green\")\n",
    "plt.plot(labels_1, color=\"red\")\n",
    "plt.plot(labels_2, color=\"orange\")\n",
    "plt.plot(labels_3, color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving average \n",
    "# https://www.youtube.com/watch?v=r3Ulu0jZCJI\n",
    "# define a period as a day, for example a 20 days moving average, take value of 20 days and adding together / count\n",
    "# this result is used as a value for the first day of that 20 days \n",
    "# when moving forward a day, drop the current first day and include a new day followed the current last day\n",
    "# and do the calculation for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e27853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting way of using markov chain\n",
    "# https://www.youtube.com/watch?v=sdp49vTanSk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885502b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first order matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6276603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning data: make continuous data into categorical data\n",
    "# https://www.youtube.com/watch?v=iv_ec0EfXcE\n",
    "\n",
    "# equal frequency binding in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://setosa.io/blog/2014/07/26/markov-chains/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254efb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=WT6jI8UgROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012aa54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
